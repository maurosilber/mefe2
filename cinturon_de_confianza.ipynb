{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herramienta para la construcción de cinturones de confianza frecuentista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import scipy.optimize as opt\n",
    "import seaborn\n",
    "import tqdm\n",
    "\n",
    "tqdm.tqdm.pandas()\n",
    "\n",
    "plt.rc(\"figure\", dpi=100)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "N_exp = 100\n",
    "N_tau = 30\n",
    "\n",
    "taus = pd.Series(name=\"scale\", data=np.linspace(0.01, 10, N_tau))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parte A\n",
    "\n",
    "1. Generar $n$ números aleatorios $x_i$ con distribución exponencial con un valor dado de $\\tau$ y calcular un estadístico $t$ igual al promedio de los $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_sample(\n",
    "    statistic: callable,\n",
    "    distribution: st.rv_continuous,\n",
    "    *,\n",
    "    n_samples: int,\n",
    ") -> float:\n",
    "    \"\"\"Calcula el dado estadístico sobre un conjunto de muestras de una dada distribución.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    statistic : callable\n",
    "        Función que toma un np.ndarray y devuelve un escalar.\n",
    "    distribution : st.rv_continuous\n",
    "        Distribución de donde se toman las muestras.\n",
    "    n_samples : int\n",
    "        Cantidad de muestras a tomar.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Valor del estadistico.\n",
    "    \"\"\"\n",
    "    samples = distribution.rvs(size=n_samples)\n",
    "    return statistic(samples)\n",
    "\n",
    "\n",
    "statistic_sample(statistic=np.mean, distribution=st.expon(scale=5), n_samples=n_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Repetir el punto anterior, un numero $N_{experimentos}$ de veces y guardar el valor del estadístico $t$ obtenido cada vez en un histograma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.histplot(\n",
    "    [\n",
    "        statistic_sample(np.mean, st.expon(scale=5), n_samples=n_samples)\n",
    "        for _ in range(N_exp)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Para ese histograma, encontrar un valor de $t_{min}$ y $t_{max}$ tal que entre ellos se encuentre una fracción CL del total de los eventos.\n",
    "\n",
    "En lugar de usar el histograma anterior, tomo directamente los percentiles 16 y 84 de un conjunto de $N_{experimentos}$, para obtener el intervalo correspondiente al 68% de las muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_quantiles_montecarlo(\n",
    "    statistic: callable,\n",
    "    distribution: st.rv_continuous,\n",
    "    *,\n",
    "    n_samples: int,\n",
    "    n_experiments: int,\n",
    "    quantiles=(0.16, 0.84),\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calcula los cuantiles de un dado estadistico a través de una simulación\n",
    "    Monte Carlo.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    statistic : callable\n",
    "        Función que toma un np.ndarray y devuelve un escalar.\n",
    "    distribution : st.rv_continuous\n",
    "        Distribución de donde se toman las muestras.\n",
    "    n_samples : int\n",
    "        Cantidad de muestras a tomar.\n",
    "    n_experiments : int\n",
    "        Número de experimentos de Monte Carlo a realizar.\n",
    "    quantiles : array-like, optional\n",
    "        Cuantiles a calcular, por defecto (0.16, 0.84)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "    \"\"\"\n",
    "\n",
    "    samples = np.empty(n_experiments)\n",
    "    for i in range(n_experiments):\n",
    "        samples[i] = statistic_sample(statistic, distribution, n_samples=n_samples)\n",
    "    statistic_quantiles = np.quantile(samples, quantiles)\n",
    "    return pd.Series(data=statistic_quantiles, index=quantiles)\n",
    "\n",
    "\n",
    "statistic_quantiles_montecarlo(\n",
    "    statistic=np.mean,\n",
    "    distribution=st.expon(scale=5),\n",
    "    n_samples=n_samples,\n",
    "    n_experiments=N_exp,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Repetir los tres puntos anteriores para 100 valores de $\\tau$ entre $0$ y $10$ y graficar el cinturón de confianza en el plano $\\tau$ vs $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_belt(\n",
    "    statistic: callable,\n",
    "    distribution: st.rv_continuous,\n",
    "    parameter: pd.Series,\n",
    "    *,\n",
    "    n_samples: int,\n",
    "    n_experiments: int,\n",
    "    quantiles=(0.16, 0.84),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Calcula el cinturon de confianza para un dado estadístico.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    statistic : callable\n",
    "        Función que toma un np.ndarray y devuelve un escalar.\n",
    "    distribution : st.rv_continuous\n",
    "        Distribución de donde se toman las muestras.\n",
    "    parameter : pd.Series\n",
    "        Parámetros para los que evaluar la distribución. El nombre\n",
    "        de la serie de Pandas debe ser el nombre del parámetro en\n",
    "        la función `distribution`.\n",
    "    n_samples : int\n",
    "        Cantidad de muestras a tomar.\n",
    "    n_experiments : int\n",
    "        Número de experimentos de Monte Carlo a realizar.\n",
    "    quantiles : array-like, optional\n",
    "        Cuantiles a calcular, por defecto (0.16, 0.84)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "    return parameter.progress_apply(\n",
    "        lambda p: statistic_quantiles_montecarlo(\n",
    "            statistic,\n",
    "            distribution(**{parameter.name: p}),\n",
    "            n_samples=n_samples,\n",
    "            n_experiments=n_experiments,\n",
    "            quantiles=quantiles,\n",
    "        )\n",
    "    ).set_index(parameter)\n",
    "\n",
    "\n",
    "belt_expon_mean = confidence_belt(\n",
    "    statistic=np.mean,\n",
    "    distribution=st.expon,\n",
    "    parameter=taus,\n",
    "    n_samples=n_samples,\n",
    "    n_experiments=N_exp,\n",
    "    quantiles=(0.16, 0.5, 0.84),\n",
    ")\n",
    "\n",
    "belt_expon_mean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belt_expon_mean.plot()\n",
    "plt.plot([0, 10], [0, 10], color=\"k\", label=\"y=x\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"Parámetro $\\tau$\")\n",
    "plt.ylabel(\"Estadístico promedio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cinturón de confianza está (aproximadamente) centrado en la recta identidad, que es el valor ideal para un estimador del parámetro. Como el promedio es el estimador de máxima verosimilitud para el parámetro $\\tau$ de la exponencial, su esperanza debería coincidir con la recta $y=x$. No necesariamente su mediana, ya que la distribución puede no ser simétrica. Se ve que esta (en naranja) va ligeramente por debajo de la recta negra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte B\n",
    "\n",
    "Utilizar la herramienta generada en la Parte A para otros dos estadísticos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. la mediana "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belt_expon_median = confidence_belt(\n",
    "    statistic=np.median,\n",
    "    distribution=st.expon,\n",
    "    parameter=taus,\n",
    "    n_samples=n_samples,\n",
    "    n_experiments=N_exp,\n",
    "    quantiles=(0.16, 0.5, 0.84),\n",
    ")\n",
    "\n",
    "belt_expon_median.plot()\n",
    "plt.plot([0, 10], [0, 10], color=\"k\", label=\"y=x\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"Parámetro $\\tau$\")\n",
    "plt.ylabel(\"Estadístico mediana\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $t = \\sum_i (x_i + x_i^2 + x_i^3 + x_i^4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powersum_statistic(x):\n",
    "    return sum(np.sum(x ** i) for i in range(1, 5))\n",
    "\n",
    "\n",
    "belt_expon_powersum = confidence_belt(\n",
    "    statistic=powersum_statistic,\n",
    "    distribution=st.expon,\n",
    "    parameter=taus,\n",
    "    n_samples=n_samples,\n",
    "    n_experiments=N_exp,\n",
    "    quantiles=(0.16, 0.5, 0.84),\n",
    ")\n",
    "\n",
    "belt_expon_powersum.plot()\n",
    "plt.plot([0, 10], [0, 10], color=\"k\", label=\"y=x\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"Parámetro $\\tau$\")\n",
    "plt.ylabel(r\"Estadístico $\\sum_i (x_i + x_i^2 + x_i^3 + x_i^4)$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte C\n",
    "Discutir que pasa si como \"estadístico\" $t$ utilizamos el $q$ de Wilks visto en clase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El $q$ de Wilks está definido como:\n",
    "\n",
    "$$ q = -2 \\ln \\frac{ \\mathcal{L}(y \\,|\\, \\theta) }{ \\mathcal{L}(y \\,|\\, \\hat{\\theta}) } $$\n",
    "\n",
    "donde $\\mathcal{L}(y \\,|\\, \\theta)$ es la verosímilitud, y $\\hat{\\theta}$ es el estimador de máxima verosimilitud.\n",
    "\n",
    "Hay dos problemas al querer calcular un cinturon de confianza a partir de esta magnitud:\n",
    "\n",
    "1. El $q$ de Wilks no es un estadístico, ya que un estadístico solo puede ser función de los datos $y$. En cambio, $q$ depende explícitamente del parámetro $\\theta$.\n",
    "\n",
    "2. Mientras que está magnitud depende explícitamente del parámetro, su distribución no depende (en el límite asintótico) de este parámetro.\n",
    "\n",
    "Por lo tanto, esta magnitud no nos sirve para calcular un cinturon de confianza.\n",
    "\n",
    "En el límite asintótico, esta magnitud tiene una distribución $\\chi^2$ de $k$ grados de libertad, donde $k$ depende de la diferencia de dimensionalidad entre $\\theta$ y $\\hat{\\theta}$. En el caso de la distribución exponencial, donde $\\theta = \\tau$ y $\\theta_0 = \\frac{1}{N} \\sum_i x_i$, tenemos que $q$ es aproximadamente una $\\chi^2_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, podemos probar hacer el cinturón utilizando el parámetro real. Como el estadístico de Wilks depende explícitamente del parámetro, no se puede reusar directamente las funciones anteriores.\n",
    "\n",
    "Para hacer más rápida la simulación, vamos a aprovechar que, en el caso de la exponencial, el estimador de máxima verosimilitud es el promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_belt_wilks(\n",
    "    distribution: st.rv_continuous,\n",
    "    maximum_likelihood_estimator: callable,\n",
    "    parameter: pd.Series,\n",
    "    *,\n",
    "    n_samples: int,\n",
    "    n_experiments: int,\n",
    "    quantiles=(0.16, 0.5, 0.84),\n",
    "):\n",
    "    \"\"\"Cinturon de confianza para el estadístico de Wilks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distribution : st.rv_continuous\n",
    "        Distribución de donde se toman las muestras.\n",
    "    maximum_likelihood_estimator : callable\n",
    "        Estimador de máxima verosimilitud. Toma un np.ndarray y devuelve un escalar.\n",
    "    parameter : pd.Series\n",
    "        Parámetros para los que evaluar la distribución. El nombre\n",
    "        de la serie de Pandas debe ser el nombre del parámetro en\n",
    "        la función `distribution`.\n",
    "    n_samples : int\n",
    "        Cantidad de muestras a tomar.\n",
    "    n_experiments : int\n",
    "        Número de experimentos de Monte Carlo a realizar.\n",
    "    quantiles : array-like, optional\n",
    "        Cuantiles a calcular, por defecto (0.16, 0.84)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "    # La función definida antes toma un estadístico que no depende del parámetro,\n",
    "    # Para reusar el código, definimos una fución que toma el parámetro y devuelve\n",
    "    # otra función que no depende del parámetro.\n",
    "    def likelihood_ratio_statistic(p):\n",
    "        dist_fixed = distribution(**{parameter.name: p})\n",
    "\n",
    "        def statistic(x):\n",
    "            p_hat = maximum_likelihood_estimator(x)\n",
    "            dist_max = distribution(**{parameter.name: p_hat})\n",
    "            return -2 * (dist_fixed.logpdf(x).sum() - dist_max.logpdf(x).sum())\n",
    "\n",
    "        return statistic\n",
    "\n",
    "    return parameter.progress_apply(\n",
    "        lambda p: statistic_quantiles_montecarlo(\n",
    "            distribution=distribution(**{parameter.name: p}),\n",
    "            statistic=likelihood_ratio_statistic(p),\n",
    "            n_samples=n_samples,\n",
    "            n_experiments=n_experiments,\n",
    "            quantiles=quantiles,\n",
    "        )\n",
    "    ).set_index(parameter)\n",
    "\n",
    "\n",
    "belt_wilks = confidence_belt_wilks(\n",
    "    distribution=st.expon,\n",
    "    maximum_likelihood_estimator=np.mean,\n",
    "    parameter=taus,\n",
    "    n_samples=n_samples,\n",
    "    n_experiments=N_exp // 10,\n",
    "    quantiles=(0.16, 0.5, 0.84),\n",
    ")\n",
    "\n",
    "belt_wilks.plot(marker=\"o\")\n",
    "plt.xlabel(r\"Parámetro $\\tau$\")\n",
    "plt.ylabel(\"Estadístico $q$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los intervalos para esta magnitud son independientes del parámetro, por lo que para un dado estadístico, el intervalo sería $[0, \\infty)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte D\n",
    "Utilizar Wilks para calcular el intervalo de confianza para el parámetro $\\tau$ de la exponencial con un dado CL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_interval(samples):\n",
    "    dist = st.expon\n",
    "    p_hat = np.mean(samples)\n",
    "    LL_max = dist.logpdf(samples, scale=p_hat).sum()\n",
    "\n",
    "    def likelihood_ratio(p, delta=1):\n",
    "        return -2 * (dist.logpdf(samples, scale=p).sum() - LL_max) - delta\n",
    "\n",
    "    sigma = np.std(samples) / samples.size ** 0.5\n",
    "    \n",
    "    for n in range(10):\n",
    "        x1 = p_hat - n * sigma\n",
    "        if likelihood_ratio(x1) > 0:\n",
    "            break\n",
    "    lower_lim = opt.root_scalar(likelihood_ratio, x0=p_hat, x1=x1).root\n",
    "    \n",
    "    for n in range(10):\n",
    "        x1 = p_hat + n * sigma\n",
    "        if likelihood_ratio(x1) > 0:\n",
    "            break\n",
    "    upper_lim = opt.root_scalar(likelihood_ratio, x0=p_hat, x1=x1).root\n",
    "    return lower_lim, upper_lim\n",
    "\n",
    "\n",
    "samples = st.expon(scale=5).rvs(n_samples)\n",
    "likelihood_interval(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte E\n",
    "(Opcional) Calcular la cobertura de los intervalos calculados en los ítems anteriores en función del parámetro $\\tau$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular el intervalo de confianza para un dado valor del estadístico, realizamos una interpolación lineal para cada una de las curvas del cinturón de confianza calculado anteriormente. Por ejemplo, para un valor $t=5$ en el caso del promedio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(statistic: float, confidence_belt: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Calcula el intervalo de confianza interpolando linealmente el cinturón\n",
    "    de confianza.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    statistic : float\n",
    "        Valor del estadístico.\n",
    "    confidence_belt : pd.DataFrame\n",
    "        Cinturón de confianza.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "    \"\"\"\n",
    "    return confidence_belt.apply(lambda x: np.interp(statistic, x, x.index))\n",
    "\n",
    "\n",
    "confidence_interval(5, belt_expon_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde un intervalo de confianza de $68\\%$ corresponde a $[3.80, 7.28]$.\n",
    "\n",
    "Para calcular la cobertura para un dado valor del parámetro $\\tau$, generamos muestras, calculamos el estadístico y calculamos la fracción de veces que el intervalo de confianza contiene al parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(\n",
    "    statistic: callable,\n",
    "    distribution: st.rv_continuous,\n",
    "    parameter: float,\n",
    "    *,\n",
    "    belt: pd.DataFrame,\n",
    "    n_samples: int,\n",
    "    n_experiments: int,\n",
    ") -> float:\n",
    "    \"\"\"Calcula la cobertura del cinturón de confianza para un dado parámetro\n",
    "    a partir de experimentos de Monte Carlo.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    statistic : callable\n",
    "        Función que toma un np.ndarray y devuelve un escalar.\n",
    "    distribution : st.rv_continuous\n",
    "        Distribución de donde se toman las muestras.\n",
    "    belt : pd.DataFrame\n",
    "        Cinturón de confianza.\n",
    "    n_samples : int\n",
    "        Cantidad de muestras a tomar.\n",
    "    n_experiments : int\n",
    "        Número de experimentos de Monte Carlo a realizar.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    coverage = 0\n",
    "    for _ in range(n_experiments):\n",
    "        samples = distribution.rvs(n_samples)\n",
    "        ci = confidence_interval(statistic(samples), belt)\n",
    "        if ci.min() <= parameter <= ci.max():\n",
    "            coverage += 1\n",
    "\n",
    "    return coverage / n_experiments\n",
    "\n",
    "\n",
    "coverage(\n",
    "    np.mean,\n",
    "    st.expon(scale=5),\n",
    "    parameter=5,\n",
    "    belt=belt_expon_mean,\n",
    "    n_samples=n_samples,\n",
    "    n_experiments=N_exp,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage_likelihood(\n",
    "    distribution: st.rv_continuous,\n",
    "    parameter: float,\n",
    "    *,\n",
    "    n_samples: int,\n",
    "    n_experiments: int,\n",
    ") -> float:\n",
    "    \"\"\"Calcula la cobertura del intervalo confianza de Wilks\n",
    "    para un dado parámetro a partir de experimentos de Monte Carlo.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distribution : st.rv_continuous\n",
    "        Distribución de donde se toman las muestras.\n",
    "    n_samples : int\n",
    "        Cantidad de muestras a tomar.\n",
    "    n_experiments : int\n",
    "        Número de experimentos de Monte Carlo a realizar.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    coverage = 0\n",
    "    for _ in range(n_experiments):\n",
    "        samples = distribution.rvs(n_samples)\n",
    "        p_min, p_max = likelihood_interval(samples)\n",
    "        if p_min <= parameter <= p_max:\n",
    "            coverage += 1\n",
    "\n",
    "    return coverage / n_experiments\n",
    "\n",
    "\n",
    "coverage_likelihood(\n",
    "    st.expon(scale=5),\n",
    "    parameter=5,\n",
    "    n_samples=n_samples,\n",
    "    n_experiments=N_exp,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos repetir esto para distintos valores del parámetro y obtener la cobertura en función del parámetro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def belt_coverage(\n",
    "    parameters: pd.Series,\n",
    "    *,\n",
    "    statistic: callable,\n",
    "    distribution: st.rv_continuous,\n",
    "    belt: pd.DataFrame,\n",
    "    n_samples: int,\n",
    "    n_experiments: int,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Cobertura del cinturon de confianza en función del parámetro.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameter : pd.Series\n",
    "        Parámetros para los que evaluar la distribución. El nombre\n",
    "        de la serie de Pandas debe ser el nombre del parámetro en\n",
    "        la función `distribution`.\n",
    "    statistic : callable\n",
    "        Función que toma un np.ndarray y devuelve un escalar.\n",
    "    distribution : st.rv_continuous\n",
    "        Distribución de donde se toman las muestras.\n",
    "    belt : pd.DataFrame\n",
    "        Cinturón de confianza.\n",
    "    n_samples : int\n",
    "        Cantidad de muestras a tomar.\n",
    "    n_experiments : int\n",
    "        Número de experimentos de Monte Carlo a realizar.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "    \"\"\"\n",
    "    parameter_coverages = parameters.progress_apply(\n",
    "        lambda p: coverage(\n",
    "            statistic=statistic,\n",
    "            distribution=distribution(**{parameters.name: p}),\n",
    "            parameter=p,\n",
    "            belt=belt,\n",
    "            n_samples=n_samples,\n",
    "            n_experiments=n_experiments,\n",
    "        )\n",
    "    )\n",
    "    return pd.Series(data=parameter_coverages.values, index=parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverages = {}\n",
    "for statistic, belt, n_samples in [\n",
    "    (np.mean, belt_expon_mean, 10),\n",
    "    (np.median, belt_expon_median, 10),\n",
    "    (powersum_statistic, belt_expon_powersum, 10),\n",
    "]:\n",
    "    coverages[statistic.__name__] = belt_coverage(\n",
    "        taus,\n",
    "        statistic=statistic,\n",
    "        distribution=st.expon,\n",
    "        belt=belt,\n",
    "        n_samples=n_samples,\n",
    "        n_experiments=N_exp // 10,\n",
    "    )\n",
    "\n",
    "coverages = pd.DataFrame(coverages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverages[\"likelihood\"] = pd.Series(data=taus.progress_apply(\n",
    "    lambda p: coverage_likelihood(\n",
    "        st.expon(scale=p),\n",
    "        parameter=p,\n",
    "        n_samples=n_samples,\n",
    "        n_experiments=N_exp // 10,\n",
    "    )\n",
    ").values, index=taus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverages.plot()\n",
    "plt.axhline(0.68, color=\"k\", label=\"Cobertura esperada\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(r\"Parámetro $\\tau$\")\n",
    "plt.ylabel(\"Cobertura\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte F\n",
    "\n",
    "(Inventado) Comparar los anchos de los intervalos obtenidos para cada estimador.\n",
    "\n",
    "Para un conjunto de $n$ muestras, calculo el ancho del intervalo correspondiente a los percentiles 16 y 84 para los tres estadísticos considerados anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval_width(statistic, belt):\n",
    "    ci = confidence_interval(statistic, belt)\n",
    "    return ci[0.16] - ci[0.84]\n",
    "\n",
    "\n",
    "def confidence_interval_widths():\n",
    "    samples = np.random.exponential(scale=5, size=n_samples)\n",
    "    return {\n",
    "        statistic.__name__: confidence_interval_width(statistic(samples), belt)\n",
    "        for statistic, belt in [\n",
    "            (np.mean, belt_expon_mean),\n",
    "            (np.median, belt_expon_median),\n",
    "            (powersum_statistic, belt_expon_powersum),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "pd.Series(confidence_interval_widths())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos esto múltiples veces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([confidence_interval_widths() for _ in tqdm.trange(N_exp)])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y comparamos las distribuciones de anchos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.histplot(df, cumulative=True, fill=False, element=\"step\")\n",
    "plt.xlabel(\"Ancho del intervalo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los anchos obtenidos por el promedio son menores a los anchos dados por los otros dos estadísticos. Llamativamente, el de la mediana es peor que el de la suma de potencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero no siempre es más chico el intervalo correspondiente al promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(lambda x, y: (x < y).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, solo el 79% de las veces, los intervalos del promedio son menores al de la mediana, y 73% al de la suma de potencias."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c98b2f5127bb22a7c793cc70443d33b89f06f1ddf2ef78e2fedd647209aac28"
  },
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('mefe': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
